{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Cheat Sheet\n",
    "This notebook is for brief snippets of helpful pandas commands that I've accumulated over time. Because I don't use pandas everyday, I've previously kept all this in an Evernote, but figured a Jupyter notebook would be more appropriate to give examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining multiple CSVs in a directory into a dataframe\n",
    "CSVs typically work a bit faster since the read_csv method has the chunksize argument, and read_excel does not. Getting filters in on each chunk before it's concatenated into a combined dataframe makes this a ton more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# if this is a script to be run from a cron job:\n",
    "# SCRIPT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "# otherwise use '.' because __file__ does not exist in a jupyter notebook\n",
    "\n",
    "SCRIPT_PATH = '.'\n",
    "DATA_FOLDER = os.path.join(SCRIPT_PATH, 'data')\n",
    "CHUNKSIZE = 10000\n",
    "\n",
    "csvs_in_dir = [os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER) if f.endswith('csv')]\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in csvs_in_dir:\n",
    "    for chunk in pd.read_csv(file, chunksize=CHUNKSIZE):\n",
    "        chunk = chunk.replace(np.nan, 'None')\n",
    "        # Add any other filters here\n",
    "        combined_df = pd.concat([combined_df, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a column to date/time\n",
    "For whatever reason, our date/time has never been formatted where pandas could auto-detect a datetime dtype. For the string representation, I typically use https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTTM_STR_FORMAT = '%m/%d/%Y %H:%M'\n",
    "\n",
    "combined_df['ACTIVITY_DT_TM'] = pd.to_datetime(combined_df['ACTIVITY_DT_TM'], format=DTTM_STR_FORMAT)\n",
    "combined_df['SERVICE_DT_TM'] = pd.to_datetime(combined_df['SERVICE_DT_TM'], format=DTTM_STR_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing column values with regex\n",
    "See also: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html\n",
    "This is helpful if you need to make sweeping changes or formatting changes to the values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not the best way:\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['ORDERED_AS_MNEMONIC.1'].replace(regex=True, inplace=True, to_replace=r' \\(ONC\\)', value=r'')b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the group by function to visualize continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.cut(df3['DIFF'], np.arange( -1830, 200, 20)))['DIFF'].count()\n",
    "# ['DIFF'] includes what columns to return\n",
    "# count() is the aggregation function\n",
    "# pd.cut separates out the continuous values by the range denoted by np.arange()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick aggregation/summarizing columns\n",
    "Summarizing can be done by creating a ```Series``` or a ```DataFrame```\n",
    "\n",
    "With a ```Series```, the column in the ```groupby``` method becomes the index, and the column in brackets is subject to the aggregation function\n",
    "\n",
    "With the ```dataframe```, we can have multiple aggregation functions in the agg method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series\n",
    "df.groupby('column_name')['column_name'].count()\n",
    "\n",
    "# Creating a dataframe\n",
    "df[['column1', 'column2']].groupby('column1').agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting values on something that has a MultiIndex\n",
    "If you have a multi-index, if you do a quick glance at your dataframe, you should see levels in your column names. You can sort things by putting the different level names into a tuple with the sort_values method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[['variety', 'price']].groupby('variety').agg(['min', 'max']).sort_values([('price', 'min'), ('price', 'max')], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
